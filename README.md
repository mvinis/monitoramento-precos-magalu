%md

# üõí Monitoramento de Pre√ßos E-commerce - Magalu

Este projeto implementa um pipeline de engenharia de dados end-to-end voltado √† coleta e an√°lise de produtos do e-commerce da Magazine Luiza. A solu√ß√£o realiza web scraping automatizado, aplica classifica√ß√£o de categorias por meio de modelos de Intelig√™ncia Artificial executados localmente e organiza os dados em um schema anal√≠tico padronizado (Schema VIP), garantindo consist√™ncia, rastreabilidade e prontid√£o para consumo em plataformas de Big Data como o Databricks.

## üï∏Ô∏è Funcionalidades

- **Web Scraping:** Utiliza Selenium com t√©cnicas de evas√£o de bot (User-Agents din√¢micos, modo incognito e exclus√£o de flags de automa√ß√£o).
- **Deep Data Extraction:** Captura dados sobre os produtos vendidos na plataforma e identifica se o produto √© de venda direta ou Marketplace (ex: Carrefour, Samsung) atrav√©s da an√°lise de metadados da URL.
- **Classifica√ß√£o com IA Local:** Utiliza o modelo `mDeBERTa-v3` (via Hugging Face Transformers) para classificar produtos em categorias sem custo de API e com alta precis√£o (Zero-Shot Classification).
- **Detec√ß√£o de Bundles:** L√≥gica inteligente para identificar combos de produtos (chamados "bundles" na mesma proposta de venda do produto - ex: Rel√≥gio + Fone), tratando falsos positivos t√©cnicos.
- **Metadata de Auditoria:** Cada registro cont√©m informa√ß√µes de vers√£o do pipeline, ambiente (dev/prod) e timestamp, garantindo linhagem de dados.
- **Schema VIP Profissional:** Estrutura de JSON aninhada que separa dados de produto, precifica√ß√£o detalhada (PIX, Cr√©dito, Parcelamento) e fontes.

## üõ†Ô∏è Tecnologias Utilizadas

- **Linguagem:** Python 3.10+
- **Automa√ß√£o:** Selenium & BeautifulSoup4
- **IA/ML:** Hugging Face Transformers & PyTorch
- **Configura√ß√£o:** Python-dotenv (Vari√°veis de Ambiente)
- **Data Lake: (Em progresso)** Integra√ß√£o com Databricks (Medallion Architecture).

## üìÅ Estrutura do Projeto

```text
‚îú‚îÄ‚îÄ data/raw/             # Arquivos JSON brutos coletados
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ classifier.py     # L√≥gica de IA para classifica√ß√£o de categorias
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py        # Tratamento de dados e montagem do Schema VIP
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py        # Motor de busca e navega√ß√£o Selenium
‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # Ferramentas auxiliares (logs, timestamps)
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (n√£o versionado)
‚îú‚îÄ‚îÄ .gitignore            # Prote√ß√£o de arquivos sens√≠veis
‚îú‚îÄ‚îÄ main.py               # Ponto de entrada da aplica√ß√£o
‚îî‚îÄ‚îÄ requirements.txt      # Depend√™ncias do projeto
```

## ‚öôÔ∏è Instru√ß√µes de Configura√ß√£o

**1. Pr√©-requisitos**

- Python instalado
- Google Chrome instalado

**2. Pr√©-requisitos**

Clone o reposit√≥rio e instale as depend√™ncias:

- `git clone [https://github.com/mvinis/monitoramento-precos-magalu.git](https://github.com/mvinis/monitoramento-precos-magalu.git)`

- `cd monitoramento-precos-magalu`

- `python -m venv venv`

- `source venv/bin/activate`
- No Windows: `.\venv\Scripts\activate`

- `pip install -r requirements.txt`

**3. Vari√°veis de Ambiente**

Crie um arquivo `.env` na raiz do projeto:

**Snippet de c√≥digo**

`PIPELINE_VERSION=v1.2`

`ENVIRONMENT=prod`

`COLLECTION_TYPE=web_scraping`

**4. Execu√ß√£o**

Para iniciar a coleta dos dados dos produtos, basta rodar:

`python main.py`

> Nota: Na primeira execu√ß√£o, o script realizar√° o download do modelo de linguagem (mDeBERTa) automaticamente. Certifique-se de ter espa√ßo em disco (~500MB) e conex√£o com a internet. O mDeBERTa √© um modelo de Intelig√™ncia Artificial treinado para entender o significado profundo de textos em diversos idiomas, inclusive o portugu√™s. Ele √© necess√°rio para analisar os nomes dos produtos e decidir, de forma inteligente e sem regras manuais (fixadas no c√≥digo), em qual categoria cada item se encaixa (ex: Smartphones, Acess√≥rios ou √Åudio).

## üß™ Qualidade e Testes

Para garantir a integridade dos dados e a resili√™ncia das transforma√ß√µes (especialmente no tratamento de valores monet√°rios brasileiros e caracteres Unicode), o projeto possui uma su√≠te de testes unit√°rios automatizados.

**1. O que √© testado?**

- **Saneamento de Moeda**: Valida√ß√£o da convers√£o de strings (ex: R$ 1.299,50) para o tipo float (1299.5).

- **Normaliza√ß√£o Unicode**: Verifica√ß√£o da remo√ß√£o de caracteres invis√≠veis (\xa0) comuns em raspagens web.

- **Resili√™ncia de Parsing**: Garantia de que entradas nulas ou inv√°lidas n√£o quebrem o pipeline (retorno padr√£o 0.0).

**2. Como rodar os testes**

Certifique-se de que o ambiente virtual est√° ativo e execute:

`python -m pytest`

> Esse comando √© necess√°rio, pois o `pytest` executa os testes a partir da pasta `tests` e, por padr√£o, n√£o reconhece a pasta `src` no `PYTHONPATH`, impedindo a importa√ß√£o das fun√ß√µes. Por isso, √© necess√°rio utilizar `python -m` no in√≠cio do comando.
