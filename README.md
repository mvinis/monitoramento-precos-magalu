%md

# üõí Monitoramento de Pre√ßos E-commerce - Magalu

Este projeto implementa um pipeline de engenharia de dados end-to-end voltado √† coleta e an√°lise de produtos do e-commerce da Magazine Luiza. A solu√ß√£o realiza web scraping automatizado, aplica classifica√ß√£o de categorias por meio de modelos de Intelig√™ncia Artificial executados localmente e organiza os dados em um schema anal√≠tico padronizado (Schema VIP), garantindo consist√™ncia, rastreabilidade e prontid√£o para consumo em plataformas de Big Data como o Databricks.

## üìê Arquitetura da Solu√ß√£o

O rob√¥ utiliza uma estrutura de **Buffer & Parsing**, garantindo que o dado seja validado e estruturado no "Schema VIP" antes mesmo de ser salvo no disco (Camada Bronze).

### Diagrama Flowchart

```mermaid
graph TD
    %% Estilos
    classDef start fill:#a5d6a7,stroke:#333,color:#000,font-weight:bold;
    classDef process fill:#90caf9,stroke:#333,color:#000,stroke-width:1px;
    classDef decision fill:#fff4dd,stroke:#d4a017,color:#000,stroke-width:2px;
    classDef error fill:#ffcdd2,stroke:#f66,color:#000,stroke-width:1px;
    classDef storage fill:#eeeeee,stroke:#333,color:#000,stroke-width:2px;
    classDef final fill:#fbb,stroke:#333,color:#000,font-weight:bold;

    Start((In√≠cio)):::start --> Config[Definir URL Inicial e<br/>Inicializar Buffer]:::process

    Config --> AccessPage[Acessar P√°gina Atual]:::process
    AccessPage --> SuccessPage{P√°gina<br/>Carregou?}:::decision

    SuccessPage -- N√£o --> LogPageErr[Log: Falha na P√°gina]:::error
    LogPageErr --> HasNext

    SuccessPage -- Sim --> FindCards[Identificar Lista de Cards]:::process

    FindCards --> CheckCards{Existem Cards<br/>pendentes?}:::decision

    CheckCards -- Sim --> PickNext[Selecionar Pr√≥ximo Card da Lista]:::process
    PickNext --> Extract[Aplicar Regras de Neg√≥cio e Schema VIP]:::process

    Extract --> ValidData{Dados<br/>V√°lidos?}:::decision

    ValidData -- Sim --> AddBuffer[Adicionar ao Buffer]:::process
    ValidData -- N√£o --> LogProdErr[Log: Pular Produto]:::error

    LogProdErr --> CheckCards
    AddBuffer --> CheckCards

    CheckCards -- N√£o --> HasNext{Existe Pr√≥xima<br/>P√°gina?}:::decision

    HasNext -- Sim --> NextURL[Preparar URL da Pr√≥xima P√°gina]:::process
    NextURL --> AccessPage

    HasNext -- N√£o --> Aggregation[Consolidar Buffer e Metadados]:::process
    Aggregation --> SaveJSON[Gerar Arquivo JSON Bronze]:::storage
    SaveJSON --> End((Fim do Processo)):::final
```

### Diagrama de Sequ√™ncia

```mermaid
sequenceDiagram
    participant ORQ as Orquestrador (Main)
    participant DRV as WebDriver (Browser)
    participant SCR as Motor de Scraping
    participant TRF as Transformador de Dados
    participant STO as Zona de Destino (Storage)
    autonumber

    ORQ->>DRV: Solicita URL Alvo
    DRV->>DRV: Aguarda Conte√∫do Din√¢mico (JS)
    DRV-->>ORQ: Conte√∫do Bruto (HTML/DOM)
    ORQ->>SCR: Analisar (Parse) Conte√∫do Bruto
    SCR->>SCR: Identifica Componentes do Produto (Nodes)
    loop Para cada Objeto de Produto
        SCR->>TRF: Envia Strings Brutas (Pre√ßo, Nome, ID)
        TRF->>TRF: Aplica Limpeza e Regras de Neg√≥cio
        TRF->>TRF: Gera Hash √önico (Fingerprint)
        TRF-->>SCR: Objeto Estruturado/Validado
    end

    SCR-->>ORQ: Dataset Normalizado
    ORQ->>STO: Persistir Dados (JSON)
```

### Diagrama de Classes

```mermaid
classDiagram
    class MagaluScraper {
        +list buffer_produtos
        +coletar_produtos()
    }

    class ProductClassifier {
        <<Service - IA Local>>
        +model mDeBERTa
        +classificar(titulo)
    }

    class ProductParsers {
        +montar_objeto_produto(dados_brutos, contexto, classificador_ai=None)
        +montar_string_bundle(base, titulo_low)
        +detectar_bundle(titulo)
    }

    class DataCleaner {
        +limpar_valor_simples_para_float(texto)
        +normalizar_texto(texto)
        +calcular_preco_total_parcelado(texto_parcela)
    }

    class DataQualityTestsParsers {
        <<UnitTests>>
        +test_limpar_valor_real_brasileiro()
        +test_normalizar_texto_com_unicode()
        +test_limpar_valor_vazio()
        +test_falsos_positivos_memoria_ram()
        +test_falsos_positivos_tecnicos()
        +test_bundles_reais()
    }

    class AILogicTests {
        <<UnitTests>>
        +test_gamepad_nao_deve_ser_smartphone(ia, contexto_padrao)
        +test_samsung_b310e_deve_ser_celular_basico(ia, contexto_padrao)
        +test_insumo_reparo_nao_deve_ser_celular(ia, contexto_padrao)
        +test_suporte_garra_nao_deve_ser_carregador(ia, contexto_padrao)
    }

    %% Rela√ß√µes com legendas (Estere√≥tipos)
    MagaluScraper --> ProductParsers : 1. Envia Cards Brutos
    ProductParsers --> DataCleaner : 2. Solicita Saneamento
    ProductParsers --> ProductClassifier : 3. Fallback Classifica via IA

    DataQualityTestsParsers ..> DataCleaner : Valida Tipagem
    DataQualityTestsParsers ..> ProductParsers : Valida Regex
    AILogicTests ..> ProductClassifier : Injeta Fixture
    AILogicTests ..> ProductParsers : Valida Fluxo H√≠brido
```

## üï∏Ô∏è Funcionalidades

- **Web Scraping:** Utiliza Selenium com t√©cnicas de evas√£o de bot (User-Agents din√¢micos, modo incognito e exclus√£o de flags de automa√ß√£o).
- **Deep Data Extraction:** Captura dados sobre os produtos vendidos na plataforma e identifica se o produto √© de venda direta ou Marketplace (ex: Carrefour, Samsung) atrav√©s da an√°lise de metadados da URL.
- **Classifica√ß√£o com IA Local:** Utiliza o modelo `mDeBERTa-v3` b√°sico (via Hugging Face Transformers) para classificar produtos em categorias sem custo de API e com alta precis√£o (Zero-Shot Classification).
- **Detec√ß√£o de Bundles:** L√≥gica inteligente para identificar combos de produtos (chamados "bundles" na mesma proposta de venda do produto - ex: Rel√≥gio + Fone), tratando falsos positivos t√©cnicos.
- **Metadata de Auditoria:** Cada registro cont√©m informa√ß√µes de vers√£o do pipeline, ambiente (dev/prod) e timestamp, garantindo linhagem de dados.
- **Schema VIP Profissional:** Estrutura de JSON aninhada que separa dados de produto, precifica√ß√£o detalhada (PIX, Cr√©dito, Parcelamento) e fontes.

## üõ†Ô∏è Tecnologias Utilizadas

- **Linguagem:** Python 3.10+
- **Automa√ß√£o:** Selenium & BeautifulSoup4
- **IA/ML:** Hugging Face Transformers & PyTorch
- **Configura√ß√£o:** Python-dotenv (Vari√°veis de Ambiente)
- **Data Lake: (Em progresso)** Integra√ß√£o com Databricks (Medallion Architecture).

## üìÅ Estrutura do Projeto

```text
‚îú‚îÄ‚îÄ data/raw/             # Arquivos JSON brutos coletados
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ classifier.py # L√≥gica de IA (NLP) para categorias
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py        # Tratamento de dados e Schema VIP
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py        # Motor de busca e navega√ß√£o Selenium
‚îÇ   ‚îî‚îÄ‚îÄ utils.py          # Ferramentas auxiliares (logs, timestamps)
‚îú‚îÄ‚îÄ tests/                # Su√≠te de testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ test_ai_logic.py  # Valida√ß√£o de infer√™ncia e categorias feita pela IA
‚îÇ   ‚îî‚îÄ‚îÄ test_parsers.py   # Valida√ß√£o de saneamento e regex
‚îú‚îÄ‚îÄ .env                  # Vari√°veis de ambiente (n√£o versionado)
‚îú‚îÄ‚îÄ .gitignore            # Prote√ß√£o de arquivos sens√≠veis
‚îú‚îÄ‚îÄ main.py               # Ponto de entrada da aplica√ß√£o
‚îî‚îÄ‚îÄ requirements.txt      # Depend√™ncias do projeto
```

## ‚öôÔ∏è Instru√ß√µes de Configura√ß√£o

**1. Pr√©-requisitos**

- Python instalado
- Google Chrome instalado

**2. Pr√©-requisitos**

Clone o reposit√≥rio e instale as depend√™ncias:

- `git clone [https://github.com/mvinis/monitoramento-precos-magalu.git](https://github.com/mvinis/monitoramento-precos-magalu.git)`

- `cd monitoramento-precos-magalu`

- `python -m venv venv`

- `source venv/bin/activate`
- No Windows: `.\venv\Scripts\activate`

- `pip install -r requirements.txt`

**3. Vari√°veis de Ambiente**

Crie um arquivo `.env` na raiz do projeto:

**Snippet de c√≥digo**

`PIPELINE_VERSION=v1.2`

`ENVIRONMENT=prod`

`COLLECTION_TYPE=web_scraping`

**4. Execu√ß√£o**

Para iniciar a coleta dos dados dos produtos, basta rodar:

`python main.py`

> Nota: Na primeira execu√ß√£o, o script realizar√° o download do modelo de linguagem (mDeBERTa) automaticamente. Certifique-se de ter espa√ßo em disco (~500MB) e conex√£o com a internet. O mDeBERTa √© um modelo de Intelig√™ncia Artificial treinado para entender o significado profundo de textos em diversos idiomas, inclusive o portugu√™s. Ele √© necess√°rio para analisar os nomes dos produtos e decidir, de forma inteligente e sem regras manuais (fixadas no c√≥digo), em qual categoria cada item se encaixa (ex: Smartphones, Acess√≥rios ou √Åudio).

## üß™ Qualidade e Testes

Para garantir a integridade dos dados e a resili√™ncia das transforma√ß√µes (especialmente no tratamento de valores monet√°rios brasileiros e caracteres Unicode), o projeto possui uma su√≠te de testes unit√°rios automatizados.

**1. O que √© testado?**

- **Saneamento de Moeda**: Valida√ß√£o da convers√£o de strings (ex: R$ 1.299,50) para o tipo float (1299.5).

- **Normaliza√ß√£o Unicode**: Verifica√ß√£o da remo√ß√£o de caracteres invis√≠veis (\xa0) comuns em raspagens web.

- **Resili√™ncia de Parsing**: Garantia de que entradas nulas ou inv√°lidas n√£o quebrem o pipeline (retorno padr√£o 0.0).

- **Blindagem de Bundles**: Valida√ß√£o de que "8GB+8GB RAM" n√£o √© detectado como combo.

- **Prioriza√ß√£o de Hardware**: Garante que "Rel√≥gio + 7 Pulseiras" mantenha a categoria 'Smartwatch'.

- **Dupla Verifica√ß√£o do resultado da IA**: Por meio do `test_ai_logic.py`, √© feito alguns testes se os produtos que eventualmente foram classificados pela IA, est√£o coerentes de fato.

**2. Como rodar os testes**

Certifique-se de que o ambiente virtual est√° ativo e execute:

`python -m pytest -v`

> Esse comando √© necess√°rio, pois o `pytest` executa os testes a partir da pasta `tests` e, por padr√£o, n√£o reconhece a pasta `src` no `PYTHONPATH`, impedindo a importa√ß√£o das fun√ß√µes. Por isso, √© necess√°rio utilizar `python -m` no in√≠cio do comando. E `-v`√© para ver as fun√ß√µes exatas de cada arquivo.
