import json
import os
from datetime import datetime
from dotenv import load_dotenv

# Importa√ß√µes internas
from src.scraper import MagaluScraper
from src.utils import configurar_logs

# 1. Carrega as configura√ß√µes do ambiente (.env)
load_dotenv()

ENV = os.getenv("ENVIRONMENT", "dev")
VERSION = os.getenv("PIPELINE_VERSION", "v1.0")

def salvar_dados(dados):

    """
    Persiste os dados coletados em formato JSON na camada local de dados brutos (raw).
    
    Cria automaticamente o diret√≥rio de destino e versiona o arquivo utilizando 
    um timestamp para evitar sobrescrita e garantir o hist√≥rico da coleta.

    Args:
        dados (list[dict]): Lista de produtos estruturados para salvar.
    """

    if not os.path.exists('data/raw'):
        os.makedirs('data/raw')
    
    timestamp_nome = datetime.now().strftime("%Y%m%d_%H%M%S")
    caminho = f'data/raw/produtos_magalu_{timestamp_nome}.json'
    
    with open(caminho, 'w', encoding='utf-8') as f:
        json.dump(dados, f, ensure_ascii=False, indent=4)
    
    print(f"\nüíæ Arquivo versionado salvo em: {caminho}")

def executar():

    """
    Orquestra o fluxo principal (workflow) da aplica√ß√£o.
    
    Respons√°vel por inicializar as configura√ß√µes de log, instanciar o motor de 
    scraping com as vari√°veis de ambiente corretas, disparar o processo de 
    coleta e garantir a persist√™ncia dos dados finais.
    """

    configurar_logs()
    
    print(f"üöÄ Iniciando extra√ß√£o | Ambiente: {ENV} | Vers√£o: {VERSION}")
    
    # 2. √â passada as vari√°veis para o bot (scraper) corretamente
    bot = MagaluScraper(ambiente=ENV, versao=VERSION)
    
    # 3. Executa a coleta
    resultados = bot.coletar_produtos()
    
    # 4. Salva o resultado final
    if resultados:
        salvar_dados(resultados)
    else:
        print("‚ö† Nenhum dado foi coletado.")

if __name__ == "__main__":
    executar()