# ğŸ›’ Monitoramento de PreÃ§os E-commerce - Magalu

Este projeto implementa um pipeline de engenharia de dados end-to-end voltado Ã  coleta e anÃ¡lise de produtos do e-commerce da Magazine Luiza. A soluÃ§Ã£o realiza web scraping automatizado, aplica classificaÃ§Ã£o de categorias por meio de modelos de InteligÃªncia Artificial executados localmente e organiza os dados em um schema analÃ­tico padronizado (Schema VIP), garantindo consistÃªncia, rastreabilidade e prontidÃ£o para consumo em plataformas de Big Data como o Databricks.

## ğŸš€ Funcionalidades

- **Web Scraping AvanÃ§ado:** Utiliza Selenium com tÃ©cnicas de evasÃ£o de bot (User-Agents dinÃ¢micos, modo incognito e exclusÃ£o de flags de automaÃ§Ã£o).
- **Deep Data Extraction:** Identifica se o produto Ã© de venda direta ou Marketplace (ex: Carrefour, Samsung) atravÃ©s da anÃ¡lise de metadados da URL.
- **ClassificaÃ§Ã£o com IA Local:** Utiliza o modelo `mDeBERTa-v3` (via Hugging Face Transformers) para classificar produtos em categorias sem custo de API e com alta precisÃ£o (Zero-Shot Classification).
- **DetecÃ§Ã£o de Bundles:** LÃ³gica inteligente para identificar combos de produtos (bundles - ex: RelÃ³gio + Fone), tratando falsos positivos tÃ©cnicos.
- **Metadata de Auditoria:** Cada registro contÃ©m informaÃ§Ãµes de versÃ£o do pipeline, ambiente (dev/prod) e timestamp, garantindo linhagem de dados.
- **Schema VIP Profissional:** Estrutura de JSON aninhada que separa dados de produto, precificaÃ§Ã£o detalhada (PIX, CrÃ©dito, Parcelamento) e fontes.

## ğŸ› ï¸ Tecnologias Utilizadas

- **Linguagem:** Python 3.10+
- **AutomaÃ§Ã£o:** Selenium & BeautifulSoup4
- **IA/ML:** Hugging Face Transformers & PyTorch
- **ConfiguraÃ§Ã£o:** Python-dotenv (VariÃ¡veis de Ambiente)
- **Data Lake: (Em progresso)** IntegraÃ§Ã£o com Databricks (Medallion Architecture).

## ğŸ“ Estrutura do Projeto

```text
â”œâ”€â”€ data/raw/             # Arquivos JSON brutos coletados
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classifier.py     # LÃ³gica de IA para classificaÃ§Ã£o de categorias
â”‚   â”œâ”€â”€ parsers.py        # Tratamento de dados e montagem do Schema VIP
â”‚   â”œâ”€â”€ scraper.py        # Motor de busca e navegaÃ§Ã£o Selenium
â”‚   â””â”€â”€ utils.py          # Ferramentas auxiliares (logs, timestamps)
â”œâ”€â”€ .env                  # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .gitignore            # ProteÃ§Ã£o de arquivos sensÃ­veis
â”œâ”€â”€ main.py               # Ponto de entrada da aplicaÃ§Ã£o
â””â”€â”€ requirements.txt      # DependÃªncias do projeto
```

âš™ï¸ InstruÃ§Ãµes de ConfiguraÃ§Ã£o

1. PrÃ©-requisitos

Python instalado.
Google Chrome instalado.

2. InstalaÃ§Ã£o
   Clone o repositÃ³rio e instale as dependÃªncias:

Bash

git clone [https://github.com/mvinis/monitoramento-precos-magalu.git](https://github.com/mvinis/monitoramento-precos-magalu.git)
cd monitoramento-precos-magalu
python -m venv venv
source venv/bin/activate # No Windows: .\venv\Scripts\activate
pip install -r requirements.txt 3. VariÃ¡veis de Ambiente
Crie um arquivo .env na raiz do projeto:

Snippet de cÃ³digo

PIPELINE_VERSION=v1.2
ENVIRONMENT=prod
COLLECTION_TYPE=web_scraping

4. ExecuÃ§Ã£o
   Para iniciar a coleta, basta rodar:

Bash

python main.py
